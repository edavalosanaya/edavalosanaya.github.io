---
---
@article{Davalos2024GazeViz,
  title = {{GazeViz}: A {Web}-{Based} {Approach} for {Visualizing} {Learner} {Gaze} {Patterns} in {Online} {Educational} {Environment}},
  language = {en},
  author = {Eduardo Davalos and Namrata Srivastava and Yike Zhang and Amanda Goodwin and Gautam Biswas},
  keywords = {Browser,Dashboard,Eye-Tracking,Scalable,Visualization},
  pdf = {ICCE2024_workshop_GazeViz.pdf},
  year = {2024},
  pubstate = {in-press},
}

@article{davalos_3d_2024,
	title = {{3D} {Gaze} {Tracking} for {Studying} {Collaborative} {Interactions} in {Mixed}-{Reality} {Environments}},
	abstract = {This study presents a novel framework for 3D gaze tracking tailored for mixed-reality settings, aimed at enhancing joint attention and collaborative efforts in team-based scenarios. Conventional gaze tracking, often limited by monocular cameras and traditional eye-tracking apparatus, struggles with simultaneous data synchronization and analysis from multiple participants in group contexts. Our proposed framework leverages state-of-the-art computer vision and machine learning techniques to overcome these obstacles, enabling precise 3D gaze estimation without dependence on specialized hardware or complex data fusion. Utilizing facial recognition and deep learning, the framework achieves real-time, tracking of gaze patterns across several individuals, addressing common depth estimation errors, and ensuring spatial and identity consistency within the dataset. Empirical results demonstrate the accuracy and reliability of our method in group environments. This provides mechanisms for significant advances in behavior and interaction analysis in educational and professional training applications in dynamic and unstructured environments.},
	language = {en},
	author = {Davalos, Eduardo and Fonteles, Joyce H and Zhang, Yike and Timalsina, Umesh},
	year = {2024},
	pdf = {ICMI_2024.pdf},
  pubstate = {in-press},
}

@article{davalos_fastposecnn_nodate,
	title = {{FastPoseCNN}: {Real}-{Time} {Monocular} {Category}-{Level} {Pose} {and} {Size} {Estimation} {Framework}},
	abstract = {The primary focus of this paper is the development of a framework for pose and size estimation of unseen objects given a single RGB image - all in real-time. In 2019, [1] proposed the first category-level pose and size estimation framework alongside two novel datasets called CAMERA and REAL. However, the novel method proposed by [1] was restricted from practical use because of its long inference time (2-4 fps). Their approach’s inference had significant delays because they used the computationally expensive MaskedRCNN framework and Umeyama algorithm. To optimize our method and yield real-time results, our framework uses the efficient ResNet-FPN framework alongside decoupling the translation, rotation, and size regression problem by using distinct decoders. Moreover, our methodology performs pose and size estimation in a global context - i.e., estimating the involved parameters of all captured objects in the image all at once. We perform extensive testing to fully compare the performance in terms of precision and speed to demonstrate the capability of our method - FastPoseCNN.},
	language = {en},
	author = {Davalos, Eduardo and Aminian, Merhan},
  year={2021},
  pdf={FastPoseCNN.pdf}
}


@article{munshi_adaptive_2022,
	title = {Adaptive {Scaffolding} to {Support} {Strategic} {Learning} in an {Open}-{Ended} {Learning} {Environment}},
	abstract = {An important goal for intelligent learning environments is to provide adaptive support for learners. This paper presents an adaptive scaffolding framework designed to support strategic learning in Betty’s Brain, an open-ended learning-by-teaching environment. The framework includes an online strategy detector that identifies students’ sub-optimal use of cognitivemetacognitive strategies, and a conversational adaptive feedback mechanism to help students overcome their difficulties. We ran a pilot study with undergraduate students to determine how well they applied the adaptive scaffolds provided by our framework. Our qualitative case study analysis used students’ activity logs, one-on-one interactions with researchers, and eye-gaze patterns on the Betty’s Brain screens, to infer their learning behaviors and performance. Findings suggest that some scaffolds helped students develop more effective and strategic behaviors to overcome their difficulties. The results also suggest a roadmap to improve scaffold design and better support long-term learning strategies in Betty’s Brain.},
	language = {en},
	author = {Munshi, Anabil and Biswas, Gautam and Davalos, Eduardo and Logan, Olivia and Rushdy, Marian},
  year = {2022},
  pdf = {Munshi2022.pdf}
}

@misc{cohn2024multimodalmethodsanalyzinglearning,
      title={Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review}, 
      author={Clayton Cohn and Eduardo Davalos and Caleb Vatral and Joyce Horn Fonteles and Hanchen David Wang and Meiyi Ma and Gautam Biswas},
      year={2024},
      eprint={2408.14491},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.14491}, 
      pdf={MultimodalMethodsSurvey.pdf}
}


@INPROCEEDINGS{10386382,
  author={Davalos, Eduardo and Timalsina, Umesh and Zhang, Yike and Wu, Jiayi and Fonteles, Joyce Horn and Biswas, Gautam},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={ChimeraPy: A Scientific Distributed Streaming Framework for Real-time Multimodal Data Retrieval and Processing}, 
  year={2023},
  volume={},
  number={},
  pages={201-206},
  keywords={Data analysis;Distributed databases;Big Data;Benchmark testing;Data transfer;Real-time systems;Behavioral sciences;multimodal;multimedia;distributed;streaming;artificial intelligence},
  doi={10.1109/BigData59044.2023.10386382},
  pdf={ChimeraPy_IEEEBigData2023.pdf},
}

@inproceedings{10.1145/3576050.3576117,
  author = {Davalos, Eduardo and Vatral, Caleb and Cohn, Clayton and Horn Fonteles, Joyce and Biswas, Gautam and Mohammed, Naveeduddin and Lee, Madison and Levin, Daniel},
  title = {Identifying Gaze Behavior Evolution via Temporal Fully-Weighted Scanpath Graphs},
  year = {2023},
  isbn = {9781450398657},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3576050.3576117},
  doi = {10.1145/3576050.3576117},
  abstract = {Eye-tracking technology has expanded our ability to quantitatively measure human perception. This rich data source has been widely used to characterize human behavior and cognition. However, eye-tracking analysis has been limited in its applicability, as contextualizing gaze to environmental artifacts is non-trivial. Moreover, the temporal evolution of gaze behavior through open-ended environments where learners are alternating between tasks often remains unclear. In this paper, we propose temporal fully-weighted scanpath graphs as a novel representation of gaze behavior and combine it with a clustering scheme to obtain high-level gaze summaries that can be mapped to cognitive tasks via network metrics and cluster mean graphs. In a case study with nurse simulation-based team training, our approach was able to explain changes in gaze behavior with respect to key events during the simulation. By identifying cognitive tasks via gaze behavior, learners’ strategies can be evaluated to create online performance metrics and personalized feedback.},
  booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
  pages = {476–487},
  numpages = {12},
  keywords = {eye-tracking, learning analytics, network analysis, simulation-based training, temporal},
  location = {<conf-loc>, <city>Arlington</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
  series = {LAK2023},
  pdf={lak2023-67.pdf}
}

@article{Vatral2022,
  title={Using the DiCoT framework for integrated multimodal analysis in mixed-reality training environments},
  author={Vatral, C. and Biswas, G. and Cohn, C. and Davalos, E. and Mohammed, N.},
  journal={Frontiers in Artificial Intelligence},
  volume={5},
  pages={941825},
  year={2022},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9353401/},
  doi={10.3389/frai.2022.941825},
  pdf={frai-05-941825.pdf}
}

@inproceedings{zhang2024monocular,
  author = {Yike Zhang and Eduardo Davalos and Dingjie Su and Ange Lou and Jack H. Noble},
  title = {{Monocular microscope to CT registration using pose estimation of the incus for augmented reality cochlear implant surgery}},
  volume = {12928},
  booktitle = {Medical Imaging 2024: Image-Guided Procedures, Robotic Interventions, and Modeling},
  editor = {Jeffrey H. Siewerdsen and Maryam E. Rettmann},
  organization = {International Society for Optics and Photonics},
  publisher = {SPIE},
  pages = {129282I},
  keywords = {Image-guided surgery, deep learning, 6D pose estimation, dense correspondence, registration},
  year = {2024},
  doi = {10.1117/12.3008830},
  URL = {https://doi.org/10.1117/12.3008830},
  pdf = {MonocularMicroscope.pdf}
}


@misc{fonteles2024step,
      title={A First Step in Using Machine Learning Methods to Enhance Interaction Analysis for Embodied Learning Environments}, 
      author={Joyce Fonteles and Eduardo Davalos and Ashwin T. S. and Yike Zhang and Mengxi Zhou and Efrat Ayalon and Alicia Lane and Selena Steinberg and Gabriella Anton and Joshua Danish and Noel Enyedy and Gautam Biswas},
      year={2024},
      eprint={2405.06203},
      archivePrefix={arXiv},
      primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'},
      arxiv={2405.06203}
}

@InProceedings{10.1007/978-3-031-36336-8_41,
  author="Vatral, Caleb
  and Lee, Madison
  and Cohn, Clayton
  and Davalos, Eduardo
  and Levin, Daniel
  and Biswas, Gautam",
  editor="Wang, Ning
  and Rebolledo-Mendez, Genaro
  and Dimitrova, Vania
  and Matsuda, Noboru
  and Santos, Olga C.",
  title="Prediction of Students' Self-confidence Using Multimodal Features in an Experiential Nurse Training Environment",
  booktitle="Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium and Blue Sky",
  year="2023",
  publisher="Springer Nature Switzerland",
  address="Cham",
  pages="266--271",
  abstract="Simulation-based experiential learning environments used in nurse training programs offer numerous advantages, including the opportunity for students to increase their self-confidence through deliberate repeated practice in a safe and controlled environment. However, measuring and monitoring students' self-confidence is challenging due to its subjective nature. In this work, we show that students' self-confidence can be predicted using multimodal data collected from the training environment. By extracting features from student eye gaze and speech patterns and combining them as inputs into a single regression model, we show that students' self-rated confidence can be predicted with high accuracy. Such predictive models may be utilized as part of a larger assessment framework designed to give instructors additional tools to support and improve student learning and patient outcomes.",
  isbn="978-3-031-36336-8",
  pdf={Vatraletal.2023-AIED.pdf}
}

@article{Vatral2023,
abstract = {Modern healthcare requires the coordination of a team of professionals with complementary skillsets. To help facilitate teamwork, healthcare professionals, such as nurses, undergo rigorous training of their clinical skills in team settings. In this paper, we analyze a mixed-reality, simulation-based training exercise involving three nurses in a hospital room. We perform multimodal interaction analysis to contrast strategies used in two cases where the patient expressed doubts about their medical care. By analyzing these strategies and comparing them to the student nurses' self-reflections, we show connections among the nurses' clinical roles, their self-efficacy, and their teamwork.},
author = {Vatral, Caleb and Cohn, Clayton and Davalos, Eduardo and Biswas, Gautam and Lee, Madison and Levin, Daniel and Hall, Eric and Holt, Jo Ellen},
doi = {10.22318/cscl2023.102810},
file = {:home/eduardo/Downloads/Vatraletal.2023-ISLS.pdf:pdf},
isbn = {9781737330684},
issn = {15734552},
journal = {Computer-Supported Collaborative Learning Conference, CSCL},
number = {June},
pages = {217--220},
title = {{A Tale of Two Nurses: Studying Groupwork in Nurse Training by Analyzing Taskwork Roles, Social Interactions, and Self-Efficacy}},
volume = {2023-June},
year = {2023},
pdf={Vatraletal.2023-ISLS.pdf}
}